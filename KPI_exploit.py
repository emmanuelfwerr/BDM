from pyspark.sql import SparkSession
from pyspark.mllib.stat import Statistics
from pyspark.mllib.tree import RandomForest
from pyspark.mllib.regression import LabeledPoint
import pyspark.sql.functions as F
from pyspark import SparkContext

<<<<<<< Updated upstream
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler, OneHotEncoder
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

=======
>>>>>>> Stashed changes

def loadMongoRDD(db, collection):
    '''
    Download data from mongodb and store it in RDD format
    '''

    spark = SparkSession \
        .builder \
        .master(f"local[*]") \
        .appName("myApp") \
        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \
        .getOrCreate()

    dataRDD = spark.read.format("mongo") \
        .option('uri', f"mongodb://10.4.41.48/{db}.{collection}") \
        .load() \
        .rdd \
        .cache()

    return dataRDD, spark

def loadMongoDF(db, collection):
    '''
    Download data from mongodb and store it in DF format
    '''
    spark = SparkSession \
        .builder \
        .master(f"local[*]") \
        .appName("myApp") \
        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \
        .getOrCreate()

    dataDF = spark.read.format("mongo") \
        .option('uri', f"mongodb://10.4.41.48/{db}.{collection}") \
        .load()

    return dataDF, spark


def mean(x, var):
    suma = 0
    num = len(x)
    for i in range(0,num):
        suma = suma + x[i][var]
    mean = suma/num

    return float("{:.2f}".format(mean))


def counter(x):
    counter = 0
    num = len(x)
    for i in range(0,num):
        counter = counter + 1

    return counter


def generateKPIs():
<<<<<<< Updated upstream

=======
    '''
    '''
>>>>>>> Stashed changes
    rdd, spark = loadMongoRDD(db='formatted', collection='nested_data')

    ## --------------- KPI 1 --------------- 
    ## to know the average of price asked for a flat/apartment per neighborhood + num apartaments to be rent/saled
    rdd1 = rdd.map(lambda x: (x['Neighborhood'], mean(x['Info Idealista'], 'price'), counter(x['Info Idealista']) ) )
    rdd1.foreach(lambda r: print(r))

    ## --------------- KPI 2 --------------- 
    ## correlación entre monthly price and RFD (family income index)
    rdd2 = rdd.map(lambda x: (x['Monthly Price (€/month)'], x['RFD most recent']))
    print(Statistics.corr(rdd2, method="pearson")) #corr of 0.98

<<<<<<< Updated upstream
    # KPI 3: modelling --> predict number of rooms given a price and neigbirhood_id

    dataDF, spark = loadMongoDF(db='formatted', collection='data')
    subsetDF = dataDF.select('Neighborhood Id', 'Price', 'Rooms') \
        .withColumnRenamed("Neighborhood Id","Neighborhood_ID")

    # Indexing 'Neighborhood_ID' (necessary for one-hot encoding)
    indexers = [StringIndexer(inputCol=column, outputCol=column+"_INDEX").fit(subsetDF) for column in ['Neighborhood_ID']]
    pipeline = Pipeline(stages=indexers) # bulky... can change to be more streamlined in final pipeline
    modelDF = pipeline.fit(subsetDF).transform(subsetDF)

    # One-Hot Encoding 'Neighborhood_ID'
    single_col_ohe = OneHotEncoder(inputCol="Neighborhood_ID_INDEX", outputCol="Neighborhood_ID_OneHot")
    modelDF_OneHot = single_col_ohe.fit(modelDF).transform(modelDF)

    # creating label index from 'Rooms' and Feature Vector from 'features'
    modelDF = StringIndexer(inputCol="Rooms", outputCol="indexedRooms") \
        .fit(modelDF_OneHot) \
        .transform(modelDF_OneHot)

    # assembling feature vector for model
    modelDF = VectorAssembler(inputCols=['Neighborhood_ID_OneHot', 'Price'], outputCol="indexedFeatures") \
        .transform(modelDF)

    # removing clutter coluns from DF
    modelDF = modelDF.select('Price', 'Rooms', 'Neighborhood_ID_OneHot', 'indexedRooms', 'indexedFeatures')

    # Split the data into training and test sets (30% held out for testing)
    (trainingDataDF, testDataDF) = modelDF.randomSplit([0.7, 0.3])

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol="indexedRooms",
                                featuresCol="indexedFeatures",
                                numTrees=3,
                                maxDepth=4,
                                maxBins=32)

    # Convert indexed labels back to original labels.
    #labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=modelDF.labels)
=======
    ## --------------- KPI 3 RDD --------------- 
    ## --> predict number of rooms given a price and neigbirhood_id
    rdd_data, spark = loadMongoRDD(db='formatted', collection='data')
    # we do one-hot-encoding for the categoric variable "Neighborhood Id"
    df1 = rdd_data.map(lambda x: (x['Rooms'], x['Price'], x['Neighborhood Id'])).toDF(['Rooms', 'Price','Neighborhood Id'])
    categories_id = df1.select('Neighborhood Id').distinct().rdd.flatMap(lambda x: x).collect()
    exprs = [F.when(F.col('Neighborhood Id') == category, 1).otherwise(0).alias(category) for category in categories_id]
    df2 = df1.select('Rooms','Price', *exprs)
    # we store the df with the hot encode done to a RDD again
    rdd3 = df2.rdd
    
    # we start the model
    labelRDD = rdd3.map(lambda x: LabeledPoint(x[0], [x[1:]]))
    (trainingData, testData) = labelRDD.randomSplit([0.7, 0.3], seed=42)

    numClasses = len(df1.select('Rooms').distinct().rdd.flatMap(lambda x: x).collect()) + 1
    model = RandomForest.trainClassifier(trainingData, 
                                            numClasses=numClasses, 
                                            categoricalFeaturesInfo={}, 
                                            numTrees=3, 
                                            featureSubsetStrategy="auto", 
                                            impurity='gini', 
                                            maxDepth=4, 
                                            maxBins=32)
>>>>>>> Stashed changes

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[rf])

    # Train model.  This also runs the indexers.
    model = pipeline.fit(trainingDataDF)

    # Make predictions.
    predictions = model.transform(testDataDF)

    # Select (prediction, true label) and compute test error
    evaluator = MulticlassClassificationEvaluator(
        labelCol="indexedRooms", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)
    print("Test Error = %g" % (1.0 - accuracy)) #0.614

    rfModel = model.stages
    print(rfModel)  # summary only
    model.write().overwrite().save('exploitation/modelRF')
