from pyspark.sql import SparkSession
from pyspark.mllib.stat import Statistics
from pyspark.mllib.tree import RandomForest
from pyspark.mllib.regression import LabeledPoint
import pyspark.sql.functions as F
from pyspark import SparkContext

def loadMongoRDD(db, collection):
    '''
    Download data from mongodb and store it in RDD format
    '''

    spark = SparkSession \
        .builder \
        .master(f"local[*]") \
        .appName("myApp") \
        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \
        .getOrCreate()

    dataRDD = spark.read.format("mongo") \
        .option('uri', f"mongodb://10.4.41.48/{db}.{collection}") \
        .load() \
        .rdd \
        .cache()

    return dataRDD, spark


def mean(x, var):
    suma = 0
    num = len(x)
    for i in range(0,num):
        suma = suma + x[i][var]
    mean = suma/num

    return float("{:.2f}".format(mean))


def counter(x):
    counter = 0
    num = len(x)
    for i in range(0,num):
        counter = counter + 1

    return counter


def generateKPIs():


    rdd, spark = loadMongoRDD(db='formatted', collection='nested_data')

    # KPI 1: to know the average of price asked for a flat/apartment per neighborhood + num apartaments to be rent/saled
    rdd1 = rdd.map(lambda x: (x['Neighborhood'], mean(x['Info Idealista'], 'price'), counter(x['Info Idealista'])))
    rdd1.foreach(lambda r: print(r))

    # KPI 2: correlación entre monthly price and RFD (family income index)
    rdd2 = rdd.map(lambda x: (x['Monthly Price (€/month)'], x['RFD most recent']))
    print(Statistics.corr(rdd2, method="pearson")) #corr of 0.98

    # KPI 3: modelling --> predict number of rooms given a price and neigbirhood_id

    rdd_data, spark = loadMongoRDD(db='formatted', collection='data')
    # we do one-hot-encode for the categoric variable "Neighborhood Id"
    df1 = rdd_data.map(lambda x: (x['Rooms'], x['Price'], x['Neighborhood Id'])).toDF(['Rooms', 'Price','Neighborhood Id'])
    categories_id = df1.select('Neighborhood Id').distinct().rdd.flatMap(lambda x: x).collect()
    exprs = [F.when(F.col('Neighborhood Id') == category, 1).otherwise(0).alias(category) for category in categories_id]
    df2 = df1.select('Rooms','Price', *exprs)
    # we store the df with the hot encode done to a RDD again
    rdd3 = df2.rdd
    # we start the model
    labelRDD = rdd3.map(lambda x: LabeledPoint(x[0], [x[1:]]))
    (trainingData, testData) = labelRDD.randomSplit([0.7, 0.3], seed=42)

    numClasses = len(df1.select('Rooms').distinct().rdd.flatMap(lambda x: x).collect()) + 1
    model = RandomForest.trainClassifier(trainingData, numClasses=numClasses, categoricalFeaturesInfo={}, numTrees=3, featureSubsetStrategy="auto", impurity='gini', maxDepth=4, maxBins=32)

    predictions = model.predict(testData.map(lambda x: x.features))

    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)

    testErr = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(testData.count())
    print('Test Error = ' + str(testErr))

    # save the final model
    spark.stop()
    sc = SparkContext(appName="rf")
    model.save(sc, 'exploitation/RFModel')

generateKPIs()
