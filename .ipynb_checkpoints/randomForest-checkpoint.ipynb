{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, Bucketizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "\n",
    "def loadMongoDF(db, collection):\n",
    "    '''\n",
    "    Download data from mongodb and store it in DF format\n",
    "    '''\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(f\"local[*]\") \\\n",
    "        .appName(\"myApp\") \\\n",
    "        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    dataDF = spark.read.format(\"mongo\") \\\n",
    "        .option('uri', f\"mongodb://10.4.41.48/{db}.{collection}\") \\\n",
    "        .load()\n",
    "\n",
    "    return dataDF, spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------- KPI 3 DF --------------- \n",
    "## --> predict number of rooms given a price and neigbirhood_id\n",
    "dataDF, spark = loadMongoDF(db='formatted', collection='data')\n",
    "subsetDF = dataDF.select('Neighborhood Id', 'Price', 'Rooms') \\\n",
    "                .withColumnRenamed(\"Neighborhood Id\",\"Neighborhood_ID\")\n",
    "\n",
    "# Indexing 'Neighborhood_ID'\n",
    "featureIndexer = [StringIndexer(inputCol=column, outputCol=column+\"_INDEX\").fit(subsetDF) for column in ['Neighborhood_ID']]\n",
    "\n",
    "#pipeline = Pipeline(stages=indexers)\n",
    "#modelDF = pipeline.fit(subsetDF).transform(subsetDF)\n",
    "\n",
    "# One-Hot Encoding 'Neighborhood_ID'\n",
    "single_col_ohe = OneHotEncoder(inputCol=\"Neighborhood_ID_INDEX\", outputCol=\"Neighborhood_ID_OneHot\").fit(subsetDF)\n",
    "#modelDF_OneHot = single_col_ohe.fit(modelDF).transform(modelDF)\n",
    "\n",
    "# creating label index from 'Rooms' and Feature Vector from 'features'\n",
    "labelIndexer = StringIndexer(inputCol=\"Rooms\", outputCol=\"indexedRooms\").fit(subsetDF)\n",
    "                #.fit(modelDF_OneHot) \\\n",
    "                #.transform(modelDF_OneHot)\n",
    "\n",
    "labelVector = VectorAssembler(inputCols=['Neighborhood_ID_OneHot', 'Price'], outputCol=\"indexedFeatures\")\n",
    "                #.transform(modelDF)\n",
    "\n",
    "#modelDF = modelDF.select('Price', 'Rooms', 'Neighborhood_ID_OneHot', 'indexedRooms', 'indexedFeatures')\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingDataDF, testDataDF) = subsetDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedRooms\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, single_col_ohe, labelIndexer, labelVector, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingDataDF)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testDataDF)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2024.load.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator$lzycompute(MongoRDD.scala:221)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator(MongoRDD.scala:221)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:68)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/efwerr/GitHub/BDM/randomForest.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39m## --------------- KPI 3 DF --------------- \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000002?line=1'>2</a>\u001b[0m \u001b[39m## --> predict number of rooms given a price and neigbirhood_id\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000002?line=2'>3</a>\u001b[0m dataDF, spark \u001b[39m=\u001b[39m loadMongoDF(db\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mformatted\u001b[39;49m\u001b[39m'\u001b[39;49m, collection\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000002?line=3'>4</a>\u001b[0m subsetDF \u001b[39m=\u001b[39m dataDF\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mNeighborhood Id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRooms\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000002?line=4'>5</a>\u001b[0m                 \u001b[39m.\u001b[39mwithColumnRenamed(\u001b[39m\"\u001b[39m\u001b[39mNeighborhood Id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mNeighborhood_ID\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000002?line=6'>7</a>\u001b[0m \u001b[39m# Indexing 'Neighborhood_ID'\u001b[39;00m\n",
      "\u001b[1;32m/Users/efwerr/GitHub/BDM/randomForest.ipynb Cell 1'\u001b[0m in \u001b[0;36mloadMongoDF\u001b[0;34m(db, collection)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=7'>8</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=8'>9</a>\u001b[0m \u001b[39mDownload data from mongodb and store it in DF format\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=9'>10</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=10'>11</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mbuilder \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39mmaster(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mmyApp\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m'\u001b[39m\u001b[39mspark.jars.packages\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morg.mongodb.spark:mongo-spark-connector_2.12:3.0.1\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=15'>16</a>\u001b[0m     \u001b[39m.\u001b[39mgetOrCreate()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=17'>18</a>\u001b[0m dataDF \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mmongo\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=18'>19</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39muri\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmongodb://10.4.41.48/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdb\u001b[39m}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m{\u001b[39;49;00mcollection\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=19'>20</a>\u001b[0m     \u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/randomForest.ipynb#ch0000000?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dataDF, spark\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/sql/readwriter.py:184\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload())\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/sql/utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    130\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2024.load.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator$lzycompute(MongoRDD.scala:221)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator(MongoRDD.scala:221)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:68)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "## --------------- KPI 3 DF --------------- \n",
    "## --> predict number of rooms given a price and neigbirhood_id\n",
    "dataDF, spark = loadMongoDF(db='formatted', collection='data')\n",
    "subsetDF = dataDF.select('Neighborhood Id', 'Price', 'Rooms') \\\n",
    "                .withColumnRenamed(\"Neighborhood Id\",\"Neighborhood_ID\")\n",
    "\n",
    "# Indexing 'Neighborhood_ID'\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_INDEX\").fit(bucketed) for column in ['Neighborhood_ID']]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "modelDF = pipeline.fit(subsetDF).transform(subsetDF)\n",
    "\n",
    "# One-Hot Encoding 'Neighborhood_ID'\n",
    "single_col_ohe = OneHotEncoder(inputCol=\"Neighborhood_ID_INDEX\", outputCol=\"Neighborhood_ID_OneHot\")\n",
    "modelDF_OneHot = single_col_ohe.fit(modelDF).transform(modelDF)\n",
    "\n",
    "# creating label index from 'Rooms' and Feature Vector from 'features'\n",
    "modelDF = StringIndexer(inputCol=\"Rooms\", outputCol=\"indexedRooms\") \\\n",
    "                .fit(modelDF_OneHot) \\\n",
    "                .transform(modelDF_OneHot)\n",
    "\n",
    "modelDF = VectorAssembler(inputCols=['Neighborhood_ID_OneHot', 'Price'], outputCol=\"indexedFeatures\") \\\n",
    "                .transform(modelDF)\n",
    "\n",
    "modelDF = modelDF.select('Price', 'Rooms', 'Neighborhood_ID_OneHot', 'indexedRooms', 'indexedFeatures')\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingDataDF, testDataDF) = modelDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedRooms\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "#labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=modelDF.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingDataDF)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testDataDF)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----+------------+\n",
      "|Neighborhood_ID|Price    |Rooms|Price_bucket|\n",
      "+---------------+---------+-----+------------+\n",
      "|Q3320722       |115000.0 |3    |2.0         |\n",
      "|Q3320722       |110000.0 |3    |2.0         |\n",
      "|Q3320722       |115000.0 |3    |2.0         |\n",
      "|Q3320722       |129000.0 |3    |2.0         |\n",
      "|Q3294602       |320000.0 |3    |4.0         |\n",
      "|Q6746220       |139500.0 |4    |2.0         |\n",
      "|Q6746220       |215000.0 |4    |3.0         |\n",
      "|Q6746220       |149000.0 |3    |2.0         |\n",
      "|Q6746220       |176000.0 |3    |2.0         |\n",
      "|Q6746220       |90000.0  |3    |1.0         |\n",
      "|Q6746220       |39000.0  |1    |1.0         |\n",
      "|Q1758503       |490000.0 |2    |5.0         |\n",
      "|Q1758503       |495000.0 |0    |5.0         |\n",
      "|Q1758503       |1125000.0|2    |12.0        |\n",
      "|Q1758503       |250000.0 |2    |3.0         |\n",
      "|Q1758503       |209000.0 |3    |3.0         |\n",
      "|Q1758503       |190000.0 |0    |2.0         |\n",
      "|Q1758503       |725000.0 |6    |8.0         |\n",
      "|Q1758503       |349000.0 |2    |4.0         |\n",
      "|Q1758503       |490000.0 |5    |5.0         |\n",
      "+---------------+---------+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ---------- 'Price' Numeric to Categorical ----------\n",
    "## only necessary for classifying price from neighborhood and rooms (which would make more sense)\n",
    "\n",
    "bucketizer = Bucketizer()\n",
    "bucketizer.setSplits([-float(\"inf\"), 0, 100000, 200000, 300000, 400000, \n",
    "                        500000, 600000, 700000, 800000, 900000, 1000000,\n",
    "                        1100000, 1200000, 1300000, 1400000, 1500000, float(\"inf\")])\n",
    "bucketizer.setInputCol(\"Price\")\n",
    "bucketizer.setOutputCol(\"Price_bucket\")\n",
    "bucketed = bucketizer.setHandleInvalid(\"keep\").transform(subsetDF)\n",
    "bucketed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"Rooms\", outputCol=\"indexedRooms\").fit(subsetDF)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"Neighborhood_ID\", outputCol=\"indexedNeighborhoodID\", maxCategories=4).fit(subsetDF)'''\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingDataDF, testDataDF) = df_indexed.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedRooms\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingDataDF)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testDataDF)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c46e04696882e2508df5460fc63f94b3349515e9904aee56ac1dfca515397f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
