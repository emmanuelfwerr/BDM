{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f76fc94",
   "metadata": {},
   "source": [
    "### Apartment price regression testing...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf04ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/17 00:44:23 WARN Utils: Your hostname, m1Mac.local resolves to a loopback address: 127.0.0.1; using 192.168.0.159 instead (on interface en0)\n",
      "22/06/17 00:44:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/efwerr/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/efwerr/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/Users/efwerr/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ebdf3d3c-a128-43a4-8426-d8c81f5f8a53;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 163ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ebdf3d3c-a128-43a4-8426-d8c81f5f8a53\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n",
      "22/06/17 00:44:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3905"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "def loadMongoRDD(spark, db, collection):\n",
    "    '''\n",
    "    Download data from mongodb and store it in RDD format\n",
    "    '''\n",
    "\n",
    "    dataRDD = spark.read.format(\"mongo\") \\\n",
    "        .option('uri', f\"mongodb://10.4.41.48/{db}.{collection}\") \\\n",
    "        .load() \\\n",
    "        .rdd \\\n",
    "        .cache()\n",
    "\n",
    "    return dataRDD\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(f\"local[*]\") \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "rdd = loadMongoRDD(spark, db='formatted', collection='data')\n",
    "rdd.take(1)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b4f7a",
   "metadata": {},
   "source": [
    "### Cheating with pyspark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a57ee0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Floor: string (nullable = false)\n",
      " |-- Bedrooms: long (nullable = true)\n",
      " |-- Rooms: long (nullable = true)\n",
      " |-- Size: double (nullable = true)\n",
      " |-- PropertyType: string (nullable = false)\n",
      " |-- District Code: string (nullable = false)\n",
      " |-- Neighborhood Code: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idealistaDF = rdd.toDF().select('Price', \n",
    "                                'Floor',\n",
    "                                'Bedrooms',\n",
    "                                'Rooms',\n",
    "                                'Size',\n",
    "                                'PropertyType',\n",
    "                                'District Code',\n",
    "                                'Neighborhood Code').fillna('unavailable')\n",
    "idealistaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "22c9ee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 929:============================================>        (168 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Floor: string (nullable = false)\n",
      " |-- Bedrooms: long (nullable = true)\n",
      " |-- Rooms: long (nullable = true)\n",
      " |-- Size: double (nullable = true)\n",
      " |-- PropertyType: string (nullable = false)\n",
      " |-- District Code: string (nullable = false)\n",
      " |-- Neighborhood Code: string (nullable = false)\n",
      " |-- Floor_7: integer (nullable = true)\n",
      " |-- Floor_en: integer (nullable = true)\n",
      " |-- Floor_-1: integer (nullable = true)\n",
      " |-- Floor_11: integer (nullable = true)\n",
      " |-- Floor_3: integer (nullable = true)\n",
      " |-- Floor_8: integer (nullable = true)\n",
      " |-- Floor_st: integer (nullable = true)\n",
      " |-- Floor_16: integer (nullable = true)\n",
      " |-- Floor_43: integer (nullable = true)\n",
      " |-- Floor_5: integer (nullable = true)\n",
      " |-- Floor_6: integer (nullable = true)\n",
      " |-- Floor_9: integer (nullable = true)\n",
      " |-- Floor_bj: integer (nullable = true)\n",
      " |-- Floor_1: integer (nullable = true)\n",
      " |-- Floor_10: integer (nullable = true)\n",
      " |-- Floor_4: integer (nullable = true)\n",
      " |-- Floor_12: integer (nullable = true)\n",
      " |-- Floor_13: integer (nullable = true)\n",
      " |-- Floor_unavailable: integer (nullable = true)\n",
      " |-- Floor_2: integer (nullable = true)\n",
      " |-- PropertyType_penthouse: integer (nullable = true)\n",
      " |-- PropertyType_duplex: integer (nullable = true)\n",
      " |-- PropertyType_studio: integer (nullable = true)\n",
      " |-- PropertyType_chalet: integer (nullable = true)\n",
      " |-- PropertyType_flat: integer (nullable = true)\n",
      " |-- PropertyType_countryHouse: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#   ##  import the required libraries\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#   ##  gather the distinct values\n",
    "distinct_values = idealistaDF.select(\"Floor\")\\\n",
    "                    .distinct()\\\n",
    "                    .rdd\\\n",
    "                    .flatMap(lambda x: x).collect()\n",
    "\n",
    "#   ##  for each of the gathered values create a new column \n",
    "for distinct_value in distinct_values:\n",
    "    function = udf(lambda item: \n",
    "                   1 if item == distinct_value else 0, \n",
    "                   IntegerType())\n",
    "    new_column_name = \"Floor\"+'_'+distinct_value\n",
    "    idealistaDF = idealistaDF.withColumn(new_column_name, function(col(\"Floor\")))\n",
    "\n",
    "#   ##  gather the distinct values\n",
    "distinct_values = idealistaDF.select(\"PropertyType\")\\\n",
    "                    .distinct()\\\n",
    "                    .rdd\\\n",
    "                    .flatMap(lambda x: x).collect()\n",
    "\n",
    "#   ##  for each of the gathered values create a new column \n",
    "for distinct_value in distinct_values:\n",
    "    function = udf(lambda item: \n",
    "                   1 if item == distinct_value else 0, \n",
    "                   IntegerType())\n",
    "    new_column_name = \"PropertyType\"+'_'+distinct_value\n",
    "    idealistaDF = idealistaDF.withColumn(new_column_name, function(col(\"PropertyType\")))\n",
    "\n",
    "idealistaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ef79a30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idealistaDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e85f835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7', 100),\n",
       " ('5', 293),\n",
       " ('2', 525),\n",
       " (None, 507),\n",
       " ('3', 500),\n",
       " ('bj', 389),\n",
       " ('4', 360),\n",
       " ('1', 779),\n",
       " ('en', 150),\n",
       " ('6', 143),\n",
       " ('st', 6),\n",
       " ('8', 90),\n",
       " ('10', 24),\n",
       " ('9', 25),\n",
       " ('16', 1),\n",
       " ('11', 4),\n",
       " ('13', 4),\n",
       " ('43', 1),\n",
       " ('-1', 2),\n",
       " ('12', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing 'Floor'\n",
    "floorRDD = rdd.map(lambda x: (x['Floor'], 1)).reduceByKey(lambda a,b: a+b)\n",
    "floorRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1aae86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sale', 3905)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing 'Operation'\n",
    "operationRDD = rdd.map(lambda x: (x['Operation'], 1)).reduceByKey(lambda a,b: a+b)\n",
    "operationRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cf15f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1811),\n",
       " (0, 2),\n",
       " (3, 335),\n",
       " (2, 1550),\n",
       " (4, 121),\n",
       " (6, 21),\n",
       " (7, 14),\n",
       " (8, 6),\n",
       " (5, 44),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing 'Bedrooms'\n",
    "bedroomsRDD = rdd.map(lambda x: (x['Bedrooms'], 1)).reduceByKey(lambda a,b: a+b)\n",
    "bedroomsRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "730dbc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1450),\n",
       " (2, 953),\n",
       " (0, 68),\n",
       " (6, 67),\n",
       " (5, 248),\n",
       " (1, 271),\n",
       " (4, 789),\n",
       " (7, 22),\n",
       " (8, 14),\n",
       " (10, 13),\n",
       " (11, 2),\n",
       " (12, 1),\n",
       " (13, 2),\n",
       " (9, 4),\n",
       " (15, 1)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing 'Rooms'\n",
    "roomsRDD = rdd.map(lambda x: (x['Rooms'], 1)).reduceByKey(lambda a,b: a+b)\n",
    "roomsRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27ef545a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sant Martí', 1),\n",
       " ('Ciutat Vella', 347),\n",
       " ('Nou Barris', 32),\n",
       " ('Sant Andreu', 10),\n",
       " ('Sants-Montjuïc', 1567),\n",
       " ('Sarrià-Sant Gervasi', 352),\n",
       " ('Horta-Guinardó', 111),\n",
       " ('Les Corts', 495),\n",
       " ('Eixample', 896),\n",
       " ('Gràcia', 94)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing 'District Code'\n",
    "districtRDD = rdd.map(lambda x: (x['District Name'], 1)).reduceByKey(lambda a,b: a+b)\n",
    "districtRDD.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ffbb1ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flat', 3340),\n",
       " ('studio', 62),\n",
       " ('penthouse', 224),\n",
       " ('chalet', 155),\n",
       " ('duplex', 123),\n",
       " ('countryHouse', 1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing 'propertyType'\n",
    "propertyRDD = rdd.map(lambda x: (x['PropertyType'], 1)).reduceByKey(lambda a,b: a+b)\n",
    "propertyRDD.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f62b657c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(320000.0, [1.0,3.0,88.0,10.0,64.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0])]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labelRDD = idealistaDF.rdd.map(lambda x: LabeledPoint(x['Price'], [x[\"Bedrooms\"], x[\"Rooms\"], x[\"Size\"], x[\"District Code\"], \n",
    "                                                        x[\"Neighborhood Code\"], x[8], x[9],\n",
    "                                                        x[10], x[11], x[12], x[13], x[14], x[15], x[16], x[17], x[18], x[19], \n",
    "                                                        x[20], x[21], x[22], x[23], x[24], x[25], x[26], x[27], x[28], x[29], \n",
    "                                                        x[30], x[31], x[32], x[33]]))\n",
    "\n",
    "\n",
    "labelRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "db94dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3905"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d203fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 208891452731.2823\n",
      "Learned regression forest model:\n",
      "TreeEnsembleModel regressor with 5 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 0 <= 3.5)\n",
      "     If (feature 2 <= 132.5)\n",
      "      If (feature 0 <= 1.5)\n",
      "       If (feature 3 <= 2.5)\n",
      "        If (feature 27 <= 0.5)\n",
      "         If (feature 29 <= 0.5)\n",
      "          If (feature 2 <= 81.5)\n",
      "           Predict: 324083.3333333333\n",
      "          Else (feature 2 > 81.5)\n",
      "           Predict: 522500.0\n",
      "         Else (feature 29 > 0.5)\n",
      "          If (feature 20 <= 0.5)\n",
      "           Predict: 343681.2921348315\n",
      "          Else (feature 20 > 0.5)\n",
      "           Predict: 299730.0\n",
      "        Else (feature 27 > 0.5)\n",
      "         If (feature 18 <= 0.5)\n",
      "          Predict: 143545.45454545456\n",
      "         Else (feature 18 > 0.5)\n",
      "          Predict: 349000.0\n",
      "       Else (feature 3 > 2.5)\n",
      "        If (feature 4 <= 18.5)\n",
      "         If (feature 1 <= 1.5)\n",
      "          If (feature 27 <= 0.5)\n",
      "           Predict: 176583.75\n",
      "          Else (feature 27 > 0.5)\n",
      "           Predict: 143026.66666666666\n",
      "         Else (feature 1 > 1.5)\n",
      "          If (feature 2 <= 65.5)\n",
      "           Predict: 212479.16666666666\n",
      "          Else (feature 2 > 65.5)\n",
      "           Predict: 249953.4818941504\n",
      "        Else (feature 4 > 18.5)\n",
      "         If (feature 29 <= 0.5)\n",
      "          If (feature 3 <= 6.5)\n",
      "           Predict: 431145.8333333333\n",
      "          Else (feature 3 > 6.5)\n",
      "           Predict: 236600.0\n",
      "         Else (feature 29 > 0.5)\n",
      "          If (feature 2 <= 71.5)\n",
      "           Predict: 229822.54901960783\n",
      "          Else (feature 2 > 71.5)\n",
      "           Predict: 348498.09523809527\n",
      "      Else (feature 0 > 1.5)\n",
      "       If (feature 4 <= 8.5)\n",
      "        If (feature 3 <= 1.5)\n",
      "         If (feature 2 <= 86.5)\n",
      "          If (feature 2 <= 72.5)\n",
      "           Predict: 590000.0\n",
      "          Else (feature 2 > 72.5)\n",
      "           Predict: 379875.0\n",
      "         Else (feature 2 > 86.5)\n",
      "          If (feature 20 <= 0.5)\n",
      "           Predict: 491593.75\n",
      "          Else (feature 20 > 0.5)\n",
      "           Predict: 753750.0\n",
      "        Else (feature 3 > 1.5)\n",
      "         If (feature 14 <= 0.5)\n",
      "          If (feature 2 <= 105.5)\n",
      "           Predict: 553880.0\n",
      "          Else (feature 2 > 105.5)\n",
      "           Predict: 744163.2653061225\n",
      "         Else (feature 14 > 0.5)\n",
      "          Predict: 539000.0\n",
      "       Else (feature 4 > 8.5)\n",
      "        If (feature 2 <= 108.5)\n",
      "         If (feature 17 <= 0.5)\n",
      "          If (feature 2 <= 93.5)\n",
      "           Predict: 363006.70250896056\n",
      "          Else (feature 2 > 93.5)\n",
      "           Predict: 438929.5081967213\n",
      "         Else (feature 17 > 0.5)\n",
      "          If (feature 4 <= 16.5)\n",
      "           Predict: 231527.77777777778\n",
      "          Else (feature 4 > 16.5)\n",
      "           Predict: 337003.5\n",
      "        Else (feature 2 > 108.5)\n",
      "         If (feature 3 <= 3.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 500105.2631578947\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 411676.3157894737\n",
      "         Else (feature 3 > 3.5)\n",
      "          If (feature 4 <= 28.5)\n",
      "           Predict: 659698.275862069\n",
      "          Else (feature 4 > 28.5)\n",
      "           Predict: 466384.6153846154\n",
      "     Else (feature 2 > 132.5)\n",
      "      If (feature 2 <= 378.0)\n",
      "       If (feature 4 <= 7.5)\n",
      "        If (feature 4 <= 6.5)\n",
      "         If (feature 2 <= 160.5)\n",
      "          If (feature 0 <= 1.5)\n",
      "           Predict: 538333.3333333334\n",
      "          Else (feature 0 > 1.5)\n",
      "           Predict: 677500.0\n",
      "         Else (feature 2 > 160.5)\n",
      "          If (feature 0 <= 2.5)\n",
      "           Predict: 822857.1428571428\n",
      "          Else (feature 0 > 2.5)\n",
      "           Predict: 1086944.4444444445\n",
      "        Else (feature 4 > 6.5)\n",
      "         If (feature 9 <= 0.5)\n",
      "          If (feature 14 <= 0.5)\n",
      "           Predict: 1422860.2150537635\n",
      "          Else (feature 14 > 0.5)\n",
      "           Predict: 2158333.3333333335\n",
      "         Else (feature 9 > 0.5)\n",
      "          If (feature 1 <= 3.5)\n",
      "           Predict: 1207500.0\n",
      "          Else (feature 1 > 3.5)\n",
      "           Predict: 724363.6363636364\n",
      "       Else (feature 4 > 7.5)\n",
      "        If (feature 1 <= 2.5)\n",
      "         If (feature 23 <= 0.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 265666.6666666667\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 557590.0\n",
      "         Else (feature 23 > 0.5)\n",
      "          Predict: 1300000.0\n",
      "        Else (feature 1 > 2.5)\n",
      "         If (feature 3 <= 4.5)\n",
      "          If (feature 4 <= 20.5)\n",
      "           Predict: 650184.7826086957\n",
      "          Else (feature 4 > 20.5)\n",
      "           Predict: 1112727.2727272727\n",
      "         Else (feature 3 > 4.5)\n",
      "          If (feature 18 <= 0.5)\n",
      "           Predict: 953221.2389380531\n",
      "          Else (feature 18 > 0.5)\n",
      "           Predict: 1101291.6666666667\n",
      "      Else (feature 2 > 378.0)\n",
      "       If (feature 2 <= 396.0)\n",
      "        If (feature 4 <= 7.5)\n",
      "         If (feature 0 <= 2.5)\n",
      "          Predict: 3480000.0\n",
      "         Else (feature 0 > 2.5)\n",
      "          Predict: 2800000.0\n",
      "        Else (feature 4 > 7.5)\n",
      "         Predict: 2960000.0\n",
      "       Else (feature 2 > 396.0)\n",
      "        If (feature 23 <= 0.5)\n",
      "         If (feature 17 <= 0.5)\n",
      "          Predict: 1900000.0\n",
      "         Else (feature 17 > 0.5)\n",
      "          Predict: 1150000.0\n",
      "        Else (feature 23 > 0.5)\n",
      "         If (feature 0 <= 1.5)\n",
      "          Predict: 3750000.0\n",
      "         Else (feature 0 > 1.5)\n",
      "          Predict: 1450000.0\n",
      "    Else (feature 0 > 3.5)\n",
      "     If (feature 1 <= 5.5)\n",
      "      If (feature 0 <= 5.5)\n",
      "       If (feature 4 <= 25.5)\n",
      "        If (feature 30 <= 0.5)\n",
      "         If (feature 1 <= 2.5)\n",
      "          Predict: 3500000.0\n",
      "         Else (feature 1 > 2.5)\n",
      "          If (feature 4 <= 19.5)\n",
      "           Predict: 1404411.7647058824\n",
      "          Else (feature 4 > 19.5)\n",
      "           Predict: 1944791.6666666667\n",
      "        Else (feature 30 > 0.5)\n",
      "         Predict: 5250000.0\n",
      "       Else (feature 4 > 25.5)\n",
      "        If (feature 29 <= 0.5)\n",
      "         If (feature 28 <= 0.5)\n",
      "          If (feature 1 <= 3.5)\n",
      "           Predict: 790000.0\n",
      "          Else (feature 1 > 3.5)\n",
      "           Predict: 719000.0\n",
      "         Else (feature 28 > 0.5)\n",
      "          If (feature 2 <= 252.5)\n",
      "           Predict: 767500.0\n",
      "          Else (feature 2 > 252.5)\n",
      "           Predict: 1240000.0\n",
      "        Else (feature 29 > 0.5)\n",
      "         If (feature 9 <= 0.5)\n",
      "          If (feature 1 <= 3.5)\n",
      "           Predict: 1280000.0\n",
      "          Else (feature 1 > 3.5)\n",
      "           Predict: 1550000.0\n",
      "         Else (feature 9 > 0.5)\n",
      "          Predict: 1650000.0\n",
      "      Else (feature 0 > 5.5)\n",
      "       If (feature 1 <= 3.5)\n",
      "        Predict: 1790000.0\n",
      "       Else (feature 1 > 3.5)\n",
      "        If (feature 23 <= 0.5)\n",
      "         If (feature 17 <= 0.5)\n",
      "          Predict: 2900000.0\n",
      "         Else (feature 17 > 0.5)\n",
      "          Predict: 3370000.0\n",
      "        Else (feature 23 > 0.5)\n",
      "         If (feature 2 <= 426.5)\n",
      "          Predict: 3500000.0\n",
      "         Else (feature 2 > 426.5)\n",
      "          Predict: 3000000.0\n",
      "     Else (feature 1 > 5.5)\n",
      "      If (feature 2 <= 675.0)\n",
      "       If (feature 3 <= 2.5)\n",
      "        If (feature 4 <= 7.5)\n",
      "         Predict: 1060000.0\n",
      "        Else (feature 4 > 7.5)\n",
      "         Predict: 1890000.0\n",
      "       Else (feature 3 > 2.5)\n",
      "        If (feature 2 <= 308.0)\n",
      "         If (feature 28 <= 0.5)\n",
      "          If (feature 4 <= 26.5)\n",
      "           Predict: 1090000.0\n",
      "          Else (feature 4 > 26.5)\n",
      "           Predict: 1265000.0\n",
      "         Else (feature 28 > 0.5)\n",
      "          If (feature 0 <= 4.5)\n",
      "           Predict: 1170000.0\n",
      "          Else (feature 0 > 4.5)\n",
      "           Predict: 1700000.0\n",
      "        Else (feature 2 > 308.0)\n",
      "         If (feature 1 <= 8.5)\n",
      "          If (feature 2 <= 450.0)\n",
      "           Predict: 2428571.4285714286\n",
      "          Else (feature 2 > 450.0)\n",
      "           Predict: 2955625.0\n",
      "         Else (feature 1 > 8.5)\n",
      "          Predict: 3734000.0\n",
      "      Else (feature 2 > 675.0)\n",
      "       If (feature 0 <= 6.5)\n",
      "        If (feature 23 <= 0.5)\n",
      "         Predict: 7450000.0\n",
      "        Else (feature 23 > 0.5)\n",
      "         Predict: 6475000.0\n",
      "       Else (feature 0 > 6.5)\n",
      "        If (feature 17 <= 0.5)\n",
      "         If (feature 1 <= 6.5)\n",
      "          Predict: 4500000.0\n",
      "         Else (feature 1 > 6.5)\n",
      "          Predict: 4600000.0\n",
      "        Else (feature 17 > 0.5)\n",
      "         Predict: 5600000.0\n",
      "  Tree 1:\n",
      "    If (feature 1 <= 4.5)\n",
      "     If (feature 29 <= 0.5)\n",
      "      If (feature 1 <= 3.5)\n",
      "       If (feature 2 <= 205.5)\n",
      "        If (feature 2 <= 96.5)\n",
      "         If (feature 2 <= 72.5)\n",
      "          If (feature 2 <= 55.5)\n",
      "           Predict: 151705.6603773585\n",
      "          Else (feature 2 > 55.5)\n",
      "           Predict: 245410.7142857143\n",
      "         Else (feature 2 > 72.5)\n",
      "          If (feature 2 <= 73.5)\n",
      "           Predict: 715000.0\n",
      "          Else (feature 2 > 73.5)\n",
      "           Predict: 354752.38095238095\n",
      "        Else (feature 2 > 96.5)\n",
      "         If (feature 0 <= 2.5)\n",
      "          If (feature 4 <= 20.5)\n",
      "           Predict: 526261.9047619047\n",
      "          Else (feature 4 > 20.5)\n",
      "           Predict: 756363.6363636364\n",
      "         Else (feature 0 > 2.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 1495000.0\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 780666.6666666666\n",
      "       Else (feature 2 > 205.5)\n",
      "        If (feature 4 <= 23.5)\n",
      "         If (feature 3 <= 2.5)\n",
      "          If (feature 4 <= 3.5)\n",
      "           Predict: 1690000.0\n",
      "          Else (feature 4 > 3.5)\n",
      "           Predict: 1900000.0\n",
      "         Else (feature 3 > 2.5)\n",
      "          If (feature 3 <= 3.5)\n",
      "           Predict: 635750.0\n",
      "          Else (feature 3 > 3.5)\n",
      "           Predict: 1150000.0\n",
      "        Else (feature 4 > 23.5)\n",
      "         If (feature 4 <= 25.5)\n",
      "          If (feature 1 <= 1.5)\n",
      "           Predict: 3750000.0\n",
      "          Else (feature 1 > 1.5)\n",
      "           Predict: 2750000.0\n",
      "         Else (feature 4 > 25.5)\n",
      "          If (feature 2 <= 222.5)\n",
      "           Predict: 850000.0\n",
      "          Else (feature 2 > 222.5)\n",
      "           Predict: 1149000.0\n",
      "      Else (feature 1 > 3.5)\n",
      "       If (feature 0 <= 3.5)\n",
      "        If (feature 2 <= 167.5)\n",
      "         If (feature 20 <= 0.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 632600.0\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 502272.7272727273\n",
      "         Else (feature 20 > 0.5)\n",
      "          Predict: 275000.0\n",
      "        Else (feature 2 > 167.5)\n",
      "         If (feature 4 <= 14.5)\n",
      "          If (feature 28 <= 0.5)\n",
      "           Predict: 1175000.0\n",
      "          Else (feature 28 > 0.5)\n",
      "           Predict: 1240000.0\n",
      "         Else (feature 4 > 14.5)\n",
      "          If (feature 17 <= 0.5)\n",
      "           Predict: 814500.0\n",
      "          Else (feature 17 > 0.5)\n",
      "           Predict: 1026666.6666666666\n",
      "       Else (feature 0 > 3.5)\n",
      "        If (feature 30 <= 0.5)\n",
      "         If (feature 2 <= 221.0)\n",
      "          Predict: 760000.0\n",
      "         Else (feature 2 > 221.0)\n",
      "          If (feature 17 <= 0.5)\n",
      "           Predict: 1670000.0\n",
      "          Else (feature 17 > 0.5)\n",
      "           Predict: 1750000.0\n",
      "        Else (feature 30 > 0.5)\n",
      "         Predict: 5250000.0\n",
      "     Else (feature 29 > 0.5)\n",
      "      If (feature 2 <= 133.5)\n",
      "       If (feature 1 <= 1.5)\n",
      "        If (feature 4 <= 7.5)\n",
      "         If (feature 3 <= 1.5)\n",
      "          If (feature 2 <= 56.5)\n",
      "           Predict: 302731.8181818182\n",
      "          Else (feature 2 > 56.5)\n",
      "           Predict: 467000.0\n",
      "         Else (feature 3 > 1.5)\n",
      "          If (feature 18 <= 0.5)\n",
      "           Predict: 391676.92307692306\n",
      "          Else (feature 18 > 0.5)\n",
      "           Predict: 652333.3333333334\n",
      "        Else (feature 4 > 7.5)\n",
      "         If (feature 0 <= 1.5)\n",
      "          If (feature 2 <= 49.1)\n",
      "           Predict: 157878.72340425532\n",
      "          Else (feature 2 > 49.1)\n",
      "           Predict: 269938.7931034483\n",
      "         Else (feature 0 > 1.5)\n",
      "          If (feature 4 <= 10.5)\n",
      "           Predict: 345000.0\n",
      "          Else (feature 4 > 10.5)\n",
      "           Predict: 179333.33333333334\n",
      "       Else (feature 1 > 1.5)\n",
      "        If (feature 17 <= 0.5)\n",
      "         If (feature 3 <= 2.5)\n",
      "          If (feature 4 <= 1.5)\n",
      "           Predict: 350793.65079365077\n",
      "          Else (feature 4 > 1.5)\n",
      "           Predict: 482307.9368932039\n",
      "         Else (feature 3 > 2.5)\n",
      "          If (feature 3 <= 3.5)\n",
      "           Predict: 272660.6923076923\n",
      "          Else (feature 3 > 3.5)\n",
      "           Predict: 404871.6\n",
      "        Else (feature 17 > 0.5)\n",
      "         If (feature 2 <= 92.5)\n",
      "          If (feature 4 <= 10.5)\n",
      "           Predict: 376000.0\n",
      "          Else (feature 4 > 10.5)\n",
      "           Predict: 247221.5909090909\n",
      "         Else (feature 2 > 92.5)\n",
      "          If (feature 1 <= 2.5)\n",
      "           Predict: 366888.8888888889\n",
      "          Else (feature 1 > 2.5)\n",
      "           Predict: 382400.0\n",
      "      Else (feature 2 > 133.5)\n",
      "       If (feature 2 <= 228.5)\n",
      "        If (feature 23 <= 0.5)\n",
      "         If (feature 4 <= 7.5)\n",
      "          If (feature 14 <= 0.5)\n",
      "           Predict: 1001225.7142857143\n",
      "          Else (feature 14 > 0.5)\n",
      "           Predict: 1950000.0\n",
      "         Else (feature 4 > 7.5)\n",
      "          If (feature 3 <= 4.5)\n",
      "           Predict: 615567.5675675676\n",
      "          Else (feature 3 > 4.5)\n",
      "           Predict: 913800.0\n",
      "        Else (feature 23 > 0.5)\n",
      "         If (feature 2 <= 200.5)\n",
      "          Predict: 1232206.896551724\n",
      "         Else (feature 2 > 200.5)\n",
      "          If (feature 0 <= 3.5)\n",
      "           Predict: 3150000.0\n",
      "          Else (feature 0 > 3.5)\n",
      "           Predict: 2750000.0\n",
      "       Else (feature 2 > 228.5)\n",
      "        If (feature 1 <= 2.5)\n",
      "         Predict: 3500000.0\n",
      "        Else (feature 1 > 2.5)\n",
      "         If (feature 0 <= 2.5)\n",
      "          If (feature 6 <= 0.5)\n",
      "           Predict: 1470555.5555555555\n",
      "          Else (feature 6 > 0.5)\n",
      "           Predict: 460000.0\n",
      "         Else (feature 0 > 2.5)\n",
      "          If (feature 2 <= 346.0)\n",
      "           Predict: 1667800.0\n",
      "          Else (feature 2 > 346.0)\n",
      "           Predict: 2837500.0\n",
      "    Else (feature 1 > 4.5)\n",
      "     If (feature 23 <= 0.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       If (feature 0 <= 2.5)\n",
      "        If (feature 24 <= 0.5)\n",
      "         If (feature 2 <= 305.5)\n",
      "          If (feature 4 <= 1.5)\n",
      "           Predict: 471000.0\n",
      "          Else (feature 4 > 1.5)\n",
      "           Predict: 737311.1111111111\n",
      "         Else (feature 2 > 305.5)\n",
      "          Predict: 3480000.0\n",
      "        Else (feature 24 > 0.5)\n",
      "         If (feature 2 <= 160.5)\n",
      "          Predict: 770000.0\n",
      "         Else (feature 2 > 160.5)\n",
      "          If (feature 1 <= 5.5)\n",
      "           Predict: 1350000.0\n",
      "          Else (feature 1 > 5.5)\n",
      "           Predict: 1050000.0\n",
      "       Else (feature 0 > 2.5)\n",
      "        If (feature 1 <= 5.5)\n",
      "         If (feature 20 <= 0.5)\n",
      "          If (feature 0 <= 4.5)\n",
      "           Predict: 939514.2857142857\n",
      "          Else (feature 0 > 4.5)\n",
      "           Predict: 2566666.6666666665\n",
      "         Else (feature 20 > 0.5)\n",
      "          If (feature 2 <= 183.5)\n",
      "           Predict: 891500.0\n",
      "          Else (feature 2 > 183.5)\n",
      "           Predict: 1350000.0\n",
      "        Else (feature 1 > 5.5)\n",
      "         If (feature 2 <= 338.5)\n",
      "          If (feature 9 <= 0.5)\n",
      "           Predict: 1410760.0\n",
      "          Else (feature 9 > 0.5)\n",
      "           Predict: 885000.0\n",
      "         Else (feature 2 > 338.5)\n",
      "          If (feature 2 <= 361.5)\n",
      "           Predict: 1800000.0\n",
      "          Else (feature 2 > 361.5)\n",
      "           Predict: 1897142.857142857\n",
      "      Else (feature 17 > 0.5)\n",
      "       If (feature 4 <= 18.5)\n",
      "        Predict: 401625.0\n",
      "       Else (feature 4 > 18.5)\n",
      "        If (feature 1 <= 6.5)\n",
      "         If (feature 4 <= 23.5)\n",
      "          Predict: 2630000.0\n",
      "         Else (feature 4 > 23.5)\n",
      "          If (feature 1 <= 5.5)\n",
      "           Predict: 3500000.0\n",
      "          Else (feature 1 > 5.5)\n",
      "           Predict: 1900000.0\n",
      "        Else (feature 1 > 6.5)\n",
      "         Predict: 5600000.0\n",
      "     Else (feature 23 > 0.5)\n",
      "      If (feature 29 <= 0.5)\n",
      "       If (feature 2 <= 655.0)\n",
      "        If (feature 0 <= 3.5)\n",
      "         If (feature 2 <= 378.0)\n",
      "          If (feature 2 <= 297.5)\n",
      "           Predict: 848333.3333333334\n",
      "          Else (feature 2 > 297.5)\n",
      "           Predict: 1450000.0\n",
      "         Else (feature 2 > 378.0)\n",
      "          Predict: 2960000.0\n",
      "        Else (feature 0 > 3.5)\n",
      "         If (feature 3 <= 4.5)\n",
      "          Predict: 3745000.0\n",
      "         Else (feature 3 > 4.5)\n",
      "          If (feature 1 <= 5.5)\n",
      "           Predict: 1814928.5714285714\n",
      "          Else (feature 1 > 5.5)\n",
      "           Predict: 2573636.3636363638\n",
      "       Else (feature 2 > 655.0)\n",
      "        If (feature 0 <= 4.5)\n",
      "         Predict: 4500000.0\n",
      "        Else (feature 0 > 4.5)\n",
      "         If (feature 1 <= 6.5)\n",
      "          If (feature 4 <= 22.0)\n",
      "           Predict: 6475000.0\n",
      "          Else (feature 4 > 22.0)\n",
      "           Predict: 6800000.0\n",
      "         Else (feature 1 > 6.5)\n",
      "          Predict: 4600000.0\n",
      "      Else (feature 29 > 0.5)\n",
      "       If (feature 3 <= 3.5)\n",
      "        Predict: 939047.619047619\n",
      "       Else (feature 3 > 3.5)\n",
      "        If (feature 1 <= 5.5)\n",
      "         Predict: 1553111.111111111\n",
      "        Else (feature 1 > 5.5)\n",
      "         Predict: 2300000.0\n",
      "  Tree 2:\n",
      "    If (feature 2 <= 378.0)\n",
      "     If (feature 1 <= 4.5)\n",
      "      If (feature 2 <= 134.5)\n",
      "       If (feature 25 <= 0.5)\n",
      "        If (feature 0 <= 1.5)\n",
      "         If (feature 2 <= 64.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 250395.28301886792\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 197389.08794788274\n",
      "         Else (feature 2 > 64.5)\n",
      "          If (feature 2 <= 85.5)\n",
      "           Predict: 284982.3728813559\n",
      "          Else (feature 2 > 85.5)\n",
      "           Predict: 352407.57575757575\n",
      "        Else (feature 0 > 1.5)\n",
      "         If (feature 17 <= 0.5)\n",
      "          If (feature 2 <= 114.5)\n",
      "           Predict: 424524.36320754717\n",
      "          Else (feature 2 > 114.5)\n",
      "           Predict: 621552.6627218935\n",
      "         Else (feature 17 > 0.5)\n",
      "          If (feature 2 <= 105.5)\n",
      "           Predict: 335614.0444444445\n",
      "          Else (feature 2 > 105.5)\n",
      "           Predict: 511173.9130434783\n",
      "       Else (feature 25 > 0.5)\n",
      "        If (feature 0 <= 1.5)\n",
      "         If (feature 4 <= 17.5)\n",
      "          If (feature 2 <= 90.5)\n",
      "           Predict: 282388.8888888889\n",
      "          Else (feature 2 > 90.5)\n",
      "           Predict: 473000.0\n",
      "         Else (feature 4 > 17.5)\n",
      "          If (feature 16 <= 0.5)\n",
      "           Predict: 423466.6666666667\n",
      "          Else (feature 16 > 0.5)\n",
      "           Predict: 450000.0\n",
      "        Else (feature 0 > 1.5)\n",
      "         If (feature 2 <= 100.5)\n",
      "          If (feature 16 <= 0.5)\n",
      "           Predict: 490666.6666666667\n",
      "          Else (feature 16 > 0.5)\n",
      "           Predict: 348333.3333333333\n",
      "         Else (feature 2 > 100.5)\n",
      "          If (feature 1 <= 1.5)\n",
      "           Predict: 1450000.0\n",
      "          Else (feature 1 > 1.5)\n",
      "           Predict: 703250.0\n",
      "      Else (feature 2 > 134.5)\n",
      "       If (feature 0 <= 2.5)\n",
      "        If (feature 16 <= 0.5)\n",
      "         If (feature 4 <= 8.5)\n",
      "          If (feature 9 <= 0.5)\n",
      "           Predict: 1105098.1818181819\n",
      "          Else (feature 9 > 0.5)\n",
      "           Predict: 698125.0\n",
      "         Else (feature 4 > 8.5)\n",
      "          If (feature 0 <= 1.5)\n",
      "           Predict: 495000.0\n",
      "          Else (feature 0 > 1.5)\n",
      "           Predict: 744938.0952380953\n",
      "        Else (feature 16 > 0.5)\n",
      "         Predict: 1795000.0\n",
      "       Else (feature 0 > 2.5)\n",
      "        If (feature 3 <= 2.5)\n",
      "         If (feature 4 <= 8.5)\n",
      "          If (feature 5 <= 0.5)\n",
      "           Predict: 1504853.3333333333\n",
      "          Else (feature 5 > 0.5)\n",
      "           Predict: 2177500.0\n",
      "         Else (feature 4 > 8.5)\n",
      "          If (feature 1 <= 3.5)\n",
      "           Predict: 1223333.3333333333\n",
      "          Else (feature 1 > 3.5)\n",
      "           Predict: 846000.0\n",
      "        Else (feature 3 > 2.5)\n",
      "         If (feature 0 <= 3.5)\n",
      "          If (feature 2 <= 228.5)\n",
      "           Predict: 801344.262295082\n",
      "          Else (feature 2 > 228.5)\n",
      "           Predict: 1229333.3333333333\n",
      "         Else (feature 0 > 3.5)\n",
      "          If (feature 4 <= 26.5)\n",
      "           Predict: 1597500.0\n",
      "          Else (feature 4 > 26.5)\n",
      "           Predict: 771250.0\n",
      "     Else (feature 1 > 4.5)\n",
      "      If (feature 0 <= 2.5)\n",
      "       If (feature 1 <= 7.5)\n",
      "        If (feature 2 <= 148.5)\n",
      "         If (feature 2 <= 126.5)\n",
      "          If (feature 1 <= 5.5)\n",
      "           Predict: 404230.76923076925\n",
      "          Else (feature 1 > 5.5)\n",
      "           Predict: 125000.0\n",
      "         Else (feature 2 > 126.5)\n",
      "          If (feature 4 <= 9.5)\n",
      "           Predict: 596000.0\n",
      "          Else (feature 4 > 9.5)\n",
      "           Predict: 455666.6666666667\n",
      "        Else (feature 2 > 148.5)\n",
      "         If (feature 2 <= 233.0)\n",
      "          If (feature 4 <= 7.5)\n",
      "           Predict: 847333.3333333334\n",
      "          Else (feature 4 > 7.5)\n",
      "           Predict: 771960.0\n",
      "         Else (feature 2 > 233.0)\n",
      "          If (feature 2 <= 243.5)\n",
      "           Predict: 1600000.0\n",
      "          Else (feature 2 > 243.5)\n",
      "           Predict: 888714.2857142857\n",
      "       Else (feature 1 > 7.5)\n",
      "        If (feature 24 <= 0.5)\n",
      "         If (feature 2 <= 255.5)\n",
      "          If (feature 2 <= 241.5)\n",
      "           Predict: 980000.0\n",
      "          Else (feature 2 > 241.5)\n",
      "           Predict: 990000.0\n",
      "         Else (feature 2 > 255.5)\n",
      "          Predict: 978000.0\n",
      "        Else (feature 24 > 0.5)\n",
      "         Predict: 950000.0\n",
      "      Else (feature 0 > 2.5)\n",
      "       If (feature 2 <= 210.5)\n",
      "        If (feature 2 <= 155.5)\n",
      "         If (feature 3 <= 4.5)\n",
      "          If (feature 2 <= 149.5)\n",
      "           Predict: 434333.3333333333\n",
      "          Else (feature 2 > 149.5)\n",
      "           Predict: 585000.0\n",
      "         Else (feature 3 > 4.5)\n",
      "          If (feature 9 <= 0.5)\n",
      "           Predict: 750000.0\n",
      "          Else (feature 9 > 0.5)\n",
      "           Predict: 510000.0\n",
      "        Else (feature 2 > 155.5)\n",
      "         If (feature 23 <= 0.5)\n",
      "          If (feature 25 <= 0.5)\n",
      "           Predict: 900312.5\n",
      "          Else (feature 25 > 0.5)\n",
      "           Predict: 1350000.0\n",
      "         Else (feature 23 > 0.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 1395000.0\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 970000.0\n",
      "       Else (feature 2 > 210.5)\n",
      "        If (feature 23 <= 0.5)\n",
      "         If (feature 4 <= 25.5)\n",
      "          If (feature 2 <= 241.5)\n",
      "           Predict: 2126666.6666666665\n",
      "          Else (feature 2 > 241.5)\n",
      "           Predict: 1148500.0\n",
      "         Else (feature 4 > 25.5)\n",
      "          If (feature 15 <= 0.5)\n",
      "           Predict: 1637000.0\n",
      "          Else (feature 15 > 0.5)\n",
      "           Predict: 1250000.0\n",
      "        Else (feature 23 > 0.5)\n",
      "         If (feature 1 <= 6.5)\n",
      "          If (feature 4 <= 26.5)\n",
      "           Predict: 1845800.0\n",
      "          Else (feature 4 > 26.5)\n",
      "           Predict: 965000.0\n",
      "         Else (feature 1 > 6.5)\n",
      "          Predict: 795000.0\n",
      "    Else (feature 2 > 378.0)\n",
      "     If (feature 3 <= 3.5)\n",
      "      If (feature 0 <= 2.5)\n",
      "       Predict: 3480000.0\n",
      "      Else (feature 0 > 2.5)\n",
      "       If (feature 29 <= 0.5)\n",
      "        If (feature 0 <= 3.5)\n",
      "         Predict: 1450000.0\n",
      "        Else (feature 0 > 3.5)\n",
      "         Predict: 985000.0\n",
      "       Else (feature 29 > 0.5)\n",
      "        If (feature 1 <= 4.5)\n",
      "         Predict: 2800000.0\n",
      "        Else (feature 1 > 4.5)\n",
      "         Predict: 1900000.0\n",
      "     Else (feature 3 > 3.5)\n",
      "      If (feature 28 <= 0.5)\n",
      "       If (feature 2 <= 450.0)\n",
      "        If (feature 0 <= 5.5)\n",
      "         If (feature 0 <= 4.5)\n",
      "          Predict: 1790000.0\n",
      "         Else (feature 0 > 4.5)\n",
      "          Predict: 2300000.0\n",
      "        Else (feature 0 > 5.5)\n",
      "         If (feature 14 <= 0.5)\n",
      "          Predict: 3500000.0\n",
      "         Else (feature 14 > 0.5)\n",
      "          Predict: 2500000.0\n",
      "       Else (feature 2 > 450.0)\n",
      "        Predict: 4000000.0\n",
      "      Else (feature 28 > 0.5)\n",
      "       If (feature 2 <= 655.0)\n",
      "        If (feature 17 <= 0.5)\n",
      "         If (feature 0 <= 5.5)\n",
      "          Predict: 3098636.3636363638\n",
      "         Else (feature 0 > 5.5)\n",
      "          If (feature 3 <= 4.5)\n",
      "           Predict: 3663333.3333333335\n",
      "          Else (feature 3 > 4.5)\n",
      "           Predict: 3186250.0\n",
      "        Else (feature 17 > 0.5)\n",
      "         Predict: 2014000.0\n",
      "       Else (feature 2 > 655.0)\n",
      "        If (feature 0 <= 4.5)\n",
      "         Predict: 3800000.0\n",
      "        Else (feature 0 > 4.5)\n",
      "         If (feature 1 <= 5.5)\n",
      "          Predict: 6900000.0\n",
      "         Else (feature 1 > 5.5)\n",
      "          If (feature 2 <= 762.0)\n",
      "           Predict: 4294000.0\n",
      "          Else (feature 2 > 762.0)\n",
      "           Predict: 6516666.666666667\n",
      "  Tree 3:\n",
      "    If (feature 1 <= 5.5)\n",
      "     If (feature 29 <= 0.5)\n",
      "      If (feature 0 <= 5.5)\n",
      "       If (feature 1 <= 3.5)\n",
      "        If (feature 2 <= 175.5)\n",
      "         If (feature 27 <= 0.5)\n",
      "          If (feature 0 <= 1.5)\n",
      "           Predict: 307527.397260274\n",
      "          Else (feature 0 > 1.5)\n",
      "           Predict: 592841.2371134021\n",
      "         Else (feature 27 > 0.5)\n",
      "          If (feature 3 <= 3.5)\n",
      "           Predict: 149770.96774193548\n",
      "          Else (feature 3 > 3.5)\n",
      "           Predict: 458333.3333333333\n",
      "        Else (feature 2 > 175.5)\n",
      "         If (feature 4 <= 23.5)\n",
      "          If (feature 4 <= 7.5)\n",
      "           Predict: 1665000.0\n",
      "          Else (feature 4 > 7.5)\n",
      "           Predict: 803076.9230769231\n",
      "         Else (feature 4 > 23.5)\n",
      "          If (feature 26 <= 0.5)\n",
      "           Predict: 3416666.6666666665\n",
      "          Else (feature 26 > 0.5)\n",
      "           Predict: 1149000.0\n",
      "       Else (feature 1 > 3.5)\n",
      "        If (feature 0 <= 2.5)\n",
      "         If (feature 2 <= 139.5)\n",
      "          If (feature 0 <= 1.5)\n",
      "           Predict: 381000.0\n",
      "          Else (feature 0 > 1.5)\n",
      "           Predict: 594230.7692307692\n",
      "         Else (feature 2 > 139.5)\n",
      "          If (feature 2 <= 210.5)\n",
      "           Predict: 696333.3333333334\n",
      "          Else (feature 2 > 210.5)\n",
      "           Predict: 763333.3333333334\n",
      "        Else (feature 0 > 2.5)\n",
      "         If (feature 2 <= 210.5)\n",
      "          If (feature 2 <= 170.5)\n",
      "           Predict: 668166.6666666666\n",
      "          Else (feature 2 > 170.5)\n",
      "           Predict: 1008666.6666666666\n",
      "         Else (feature 2 > 210.5)\n",
      "          If (feature 3 <= 5.5)\n",
      "           Predict: 1472250.0\n",
      "          Else (feature 3 > 5.5)\n",
      "           Predict: 732500.0\n",
      "      Else (feature 0 > 5.5)\n",
      "       If (feature 2 <= 613.5)\n",
      "        If (feature 15 <= 0.5)\n",
      "         Predict: 3745000.0\n",
      "        Else (feature 15 > 0.5)\n",
      "         Predict: 2900000.0\n",
      "       Else (feature 2 > 613.5)\n",
      "        If (feature 2 <= 666.0)\n",
      "         Predict: 6900000.0\n",
      "        Else (feature 2 > 666.0)\n",
      "         Predict: 4900000.0\n",
      "     Else (feature 29 > 0.5)\n",
      "      If (feature 0 <= 2.5)\n",
      "       If (feature 17 <= 0.5)\n",
      "        If (feature 3 <= 2.5)\n",
      "         If (feature 2 <= 142.5)\n",
      "          If (feature 0 <= 1.5)\n",
      "           Predict: 342480.9051724138\n",
      "          Else (feature 0 > 1.5)\n",
      "           Predict: 523024.8101265823\n",
      "         Else (feature 2 > 142.5)\n",
      "          If (feature 1 <= 2.5)\n",
      "           Predict: 1915307.6923076923\n",
      "          Else (feature 1 > 2.5)\n",
      "           Predict: 948371.4285714285\n",
      "        Else (feature 3 > 2.5)\n",
      "         If (feature 0 <= 1.5)\n",
      "          If (feature 3 <= 3.5)\n",
      "           Predict: 231813.67595818816\n",
      "          Else (feature 3 > 3.5)\n",
      "           Predict: 313419.1489361702\n",
      "         Else (feature 0 > 1.5)\n",
      "          If (feature 2 <= 127.5)\n",
      "           Predict: 417068.44170403585\n",
      "          Else (feature 2 > 127.5)\n",
      "           Predict: 794876.9230769231\n",
      "       Else (feature 17 > 0.5)\n",
      "        Predict: 279452.95705521473\n",
      "      Else (feature 0 > 2.5)\n",
      "       If (feature 23 <= 0.5)\n",
      "        If (feature 4 <= 9.5)\n",
      "         If (feature 1 <= 3.5)\n",
      "          If (feature 2 <= 115.5)\n",
      "           Predict: 536666.6666666666\n",
      "          Else (feature 2 > 115.5)\n",
      "           Predict: 1332553.1914893617\n",
      "         Else (feature 1 > 3.5)\n",
      "          If (feature 0 <= 3.5)\n",
      "           Predict: 1801052.6315789474\n",
      "          Else (feature 0 > 3.5)\n",
      "           Predict: 1186800.0\n",
      "        Else (feature 4 > 9.5)\n",
      "         If (feature 0 <= 3.5)\n",
      "          If (feature 2 <= 252.5)\n",
      "           Predict: 808470.5882352941\n",
      "          Else (feature 2 > 252.5)\n",
      "           Predict: 1653750.0\n",
      "         Else (feature 0 > 3.5)\n",
      "          If (feature 2 <= 297.5)\n",
      "           Predict: 1207857.142857143\n",
      "          Else (feature 2 > 297.5)\n",
      "           Predict: 1720000.0\n",
      "       Else (feature 23 > 0.5)\n",
      "        If (feature 3 <= 2.5)\n",
      "         If (feature 4 <= 7.5)\n",
      "          If (feature 0 <= 3.5)\n",
      "           Predict: 1750000.0\n",
      "          Else (feature 0 > 3.5)\n",
      "           Predict: 1490000.0\n",
      "         Else (feature 4 > 7.5)\n",
      "          Predict: 585000.0\n",
      "        Else (feature 3 > 2.5)\n",
      "         If (feature 4 <= 14.5)\n",
      "          If (feature 1 <= 3.5)\n",
      "           Predict: 685500.0\n",
      "          Else (feature 1 > 3.5)\n",
      "           Predict: 389000.0\n",
      "         Else (feature 4 > 14.5)\n",
      "          If (feature 2 <= 224.5)\n",
      "           Predict: 1197083.3333333333\n",
      "          Else (feature 2 > 224.5)\n",
      "           Predict: 2450000.0\n",
      "    Else (feature 1 > 5.5)\n",
      "     If (feature 2 <= 488.0)\n",
      "      If (feature 0 <= 2.5)\n",
      "       If (feature 2 <= 136.5)\n",
      "        If (feature 2 <= 121.5)\n",
      "         Predict: 125000.0\n",
      "        Else (feature 2 > 121.5)\n",
      "         Predict: 550000.0\n",
      "       Else (feature 2 > 136.5)\n",
      "        Predict: 1046666.6666666666\n",
      "      Else (feature 0 > 2.5)\n",
      "       If (feature 4 <= 23.5)\n",
      "        If (feature 25 <= 0.5)\n",
      "         If (feature 18 <= 0.5)\n",
      "          If (feature 29 <= 0.5)\n",
      "           Predict: 2072000.0\n",
      "          Else (feature 29 > 0.5)\n",
      "           Predict: 1561000.0\n",
      "         Else (feature 18 > 0.5)\n",
      "          Predict: 3050000.0\n",
      "        Else (feature 25 > 0.5)\n",
      "         Predict: 1700000.0\n",
      "       Else (feature 4 > 23.5)\n",
      "        If (feature 4 <= 24.5)\n",
      "         If (feature 2 <= 185.5)\n",
      "          Predict: 899000.0\n",
      "         Else (feature 2 > 185.5)\n",
      "          Predict: 1285000.0\n",
      "        Else (feature 4 > 24.5)\n",
      "         If (feature 0 <= 4.5)\n",
      "          If (feature 2 <= 297.5)\n",
      "           Predict: 1027857.1428571428\n",
      "          Else (feature 2 > 297.5)\n",
      "           Predict: 1586666.6666666667\n",
      "         Else (feature 0 > 4.5)\n",
      "          If (feature 1 <= 6.5)\n",
      "           Predict: 2300000.0\n",
      "          Else (feature 1 > 6.5)\n",
      "           Predict: 2200000.0\n",
      "     Else (feature 2 > 488.0)\n",
      "      If (feature 0 <= 4.5)\n",
      "       If (feature 1 <= 7.5)\n",
      "        Predict: 3500000.0\n",
      "       Else (feature 1 > 7.5)\n",
      "        If (feature 1 <= 9.5)\n",
      "         Predict: 3730000.0\n",
      "        Else (feature 1 > 9.5)\n",
      "         Predict: 3800000.0\n",
      "      Else (feature 0 > 4.5)\n",
      "       If (feature 0 <= 5.5)\n",
      "        Predict: 7450000.0\n",
      "       Else (feature 0 > 5.5)\n",
      "        If (feature 14 <= 0.5)\n",
      "         If (feature 1 <= 11.5)\n",
      "          If (feature 2 <= 675.0)\n",
      "           Predict: 3485000.0\n",
      "          Else (feature 2 > 675.0)\n",
      "           Predict: 6031250.0\n",
      "         Else (feature 1 > 11.5)\n",
      "          Predict: 3730000.0\n",
      "        Else (feature 14 > 0.5)\n",
      "         Predict: 3500000.0\n",
      "  Tree 4:\n",
      "    If (feature 0 <= 2.5)\n",
      "     If (feature 4 <= 8.5)\n",
      "      If (feature 2 <= 140.5)\n",
      "       If (feature 4 <= 6.5)\n",
      "        If (feature 2 <= 100.5)\n",
      "         If (feature 27 <= 0.5)\n",
      "          If (feature 2 <= 64.5)\n",
      "           Predict: 264087.2340425532\n",
      "          Else (feature 2 > 64.5)\n",
      "           Predict: 375260.5633802817\n",
      "         Else (feature 27 > 0.5)\n",
      "          If (feature 18 <= 0.5)\n",
      "           Predict: 148888.88888888888\n",
      "          Else (feature 18 > 0.5)\n",
      "           Predict: 349000.0\n",
      "        Else (feature 2 > 100.5)\n",
      "         If (feature 0 <= 1.5)\n",
      "          If (feature 20 <= 0.5)\n",
      "           Predict: 392590.9090909091\n",
      "          Else (feature 20 > 0.5)\n",
      "           Predict: 261000.0\n",
      "         Else (feature 0 > 1.5)\n",
      "          If (feature 1 <= 5.5)\n",
      "           Predict: 626131.5789473684\n",
      "          Else (feature 1 > 5.5)\n",
      "           Predict: 125000.0\n",
      "       Else (feature 4 > 6.5)\n",
      "        If (feature 20 <= 0.5)\n",
      "         If (feature 14 <= 0.5)\n",
      "          If (feature 23 <= 0.5)\n",
      "           Predict: 608826.6666666666\n",
      "          Else (feature 23 > 0.5)\n",
      "           Predict: 764842.1052631579\n",
      "         Else (feature 14 > 0.5)\n",
      "          If (feature 0 <= 1.5)\n",
      "           Predict: 454000.0\n",
      "          Else (feature 0 > 1.5)\n",
      "           Predict: 669500.0\n",
      "        Else (feature 20 > 0.5)\n",
      "         Predict: 493500.0\n",
      "      Else (feature 2 > 140.5)\n",
      "       If (feature 1 <= 2.5)\n",
      "        If (feature 1 <= 0.5)\n",
      "         Predict: 530000.0\n",
      "        Else (feature 1 > 0.5)\n",
      "         If (feature 16 <= 0.5)\n",
      "          If (feature 4 <= 3.5)\n",
      "           Predict: 1115000.0\n",
      "          Else (feature 4 > 3.5)\n",
      "           Predict: 2177857.1428571427\n",
      "         Else (feature 16 > 0.5)\n",
      "          Predict: 1795000.0\n",
      "       Else (feature 1 > 2.5)\n",
      "        If (feature 4 <= 3.5)\n",
      "         If (feature 0 <= 1.5)\n",
      "          Predict: 560000.0\n",
      "         Else (feature 0 > 1.5)\n",
      "          If (feature 18 <= 0.5)\n",
      "           Predict: 782500.0\n",
      "          Else (feature 18 > 0.5)\n",
      "           Predict: 678333.3333333334\n",
      "        Else (feature 4 > 3.5)\n",
      "         If (feature 9 <= 0.5)\n",
      "          If (feature 24 <= 0.5)\n",
      "           Predict: 1170459.0909090908\n",
      "          Else (feature 24 > 0.5)\n",
      "           Predict: 1346666.6666666667\n",
      "         Else (feature 9 > 0.5)\n",
      "          Predict: 948625.0\n",
      "     Else (feature 4 > 8.5)\n",
      "      If (feature 28 <= 0.5)\n",
      "       If (feature 3 <= 3.5)\n",
      "        If (feature 2 <= 80.2)\n",
      "         If (feature 27 <= 0.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 328113.3035714286\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 226598.47792998477\n",
      "         Else (feature 27 > 0.5)\n",
      "          If (feature 2 <= 40.5)\n",
      "           Predict: 103982.35294117648\n",
      "          Else (feature 2 > 40.5)\n",
      "           Predict: 175500.0\n",
      "        Else (feature 2 > 80.2)\n",
      "         If (feature 2 <= 127.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 434223.3918128655\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 347252.13473053894\n",
      "         Else (feature 2 > 127.5)\n",
      "          If (feature 2 <= 172.5)\n",
      "           Predict: 528790.625\n",
      "          Else (feature 2 > 172.5)\n",
      "           Predict: 735388.8888888889\n",
      "       Else (feature 3 > 3.5)\n",
      "        If (feature 0 <= 1.5)\n",
      "         If (feature 2 <= 64.5)\n",
      "          If (feature 19 <= 0.5)\n",
      "           Predict: 201830.50847457626\n",
      "          Else (feature 19 > 0.5)\n",
      "           Predict: 425000.0\n",
      "         Else (feature 2 > 64.5)\n",
      "          If (feature 4 <= 28.5)\n",
      "           Predict: 369971.32867132867\n",
      "          Else (feature 4 > 28.5)\n",
      "           Predict: 239164.58333333334\n",
      "        Else (feature 0 > 1.5)\n",
      "         If (feature 4 <= 20.5)\n",
      "          If (feature 4 <= 19.5)\n",
      "           Predict: 595527.027027027\n",
      "          Else (feature 4 > 19.5)\n",
      "           Predict: 401123.820754717\n",
      "         Else (feature 4 > 20.5)\n",
      "          If (feature 4 <= 26.5)\n",
      "           Predict: 924297.2972972973\n",
      "          Else (feature 4 > 26.5)\n",
      "           Predict: 432982.7586206897\n",
      "      Else (feature 28 > 0.5)\n",
      "       If (feature 1 <= 1.5)\n",
      "        If (feature 2 <= 249.5)\n",
      "         If (feature 4 <= 16.5)\n",
      "          If (feature 2 <= 43.5)\n",
      "           Predict: 147000.0\n",
      "          Else (feature 2 > 43.5)\n",
      "           Predict: 179000.0\n",
      "         Else (feature 4 > 16.5)\n",
      "          Predict: 750000.0\n",
      "        Else (feature 2 > 249.5)\n",
      "         Predict: 3750000.0\n",
      "       Else (feature 1 > 1.5)\n",
      "        If (feature 3 <= 5.5)\n",
      "         If (feature 4 <= 14.5)\n",
      "          If (feature 2 <= 120.5)\n",
      "           Predict: 330200.0\n",
      "          Else (feature 2 > 120.5)\n",
      "           Predict: 1075000.0\n",
      "         Else (feature 4 > 14.5)\n",
      "          If (feature 2 <= 161.5)\n",
      "           Predict: 350000.0\n",
      "          Else (feature 2 > 161.5)\n",
      "           Predict: 1174000.0\n",
      "        Else (feature 3 > 5.5)\n",
      "         If (feature 3 <= 7.5)\n",
      "          Predict: 394666.6666666667\n",
      "         Else (feature 3 > 7.5)\n",
      "          Predict: 110000.0\n",
      "    Else (feature 0 > 2.5)\n",
      "     If (feature 1 <= 5.5)\n",
      "      If (feature 30 <= 0.5)\n",
      "       If (feature 23 <= 0.5)\n",
      "        If (feature 0 <= 4.5)\n",
      "         If (feature 5 <= 0.5)\n",
      "          If (feature 1 <= 3.5)\n",
      "           Predict: 1062597.7011494252\n",
      "          Else (feature 1 > 3.5)\n",
      "           Predict: 1240798.1651376146\n",
      "         Else (feature 5 > 0.5)\n",
      "          If (feature 29 <= 0.5)\n",
      "           Predict: 1350000.0\n",
      "          Else (feature 29 > 0.5)\n",
      "           Predict: 1995000.0\n",
      "        Else (feature 0 > 4.5)\n",
      "         If (feature 4 <= 3.5)\n",
      "          If (feature 26 <= 0.5)\n",
      "           Predict: 1890000.0\n",
      "          Else (feature 26 > 0.5)\n",
      "           Predict: 1690000.0\n",
      "         Else (feature 4 > 3.5)\n",
      "          If (feature 3 <= 4.5)\n",
      "           Predict: 1230000.0\n",
      "          Else (feature 3 > 4.5)\n",
      "           Predict: 1700000.0\n",
      "       Else (feature 23 > 0.5)\n",
      "        If (feature 3 <= 5.5)\n",
      "         If (feature 25 <= 0.5)\n",
      "          If (feature 1 <= 4.5)\n",
      "           Predict: 1386947.3684210526\n",
      "          Else (feature 1 > 4.5)\n",
      "           Predict: 2381312.5\n",
      "         Else (feature 25 > 0.5)\n",
      "          If (feature 4 <= 24.5)\n",
      "           Predict: 968000.0\n",
      "          Else (feature 4 > 24.5)\n",
      "           Predict: 1440000.0\n",
      "        Else (feature 3 > 5.5)\n",
      "         If (feature 1 <= 4.5)\n",
      "          If (feature 2 <= 170.5)\n",
      "           Predict: 745000.0\n",
      "          Else (feature 2 > 170.5)\n",
      "           Predict: 775000.0\n",
      "         Else (feature 1 > 4.5)\n",
      "          If (feature 2 <= 210.5)\n",
      "           Predict: 636333.3333333334\n",
      "          Else (feature 2 > 210.5)\n",
      "           Predict: 1495000.0\n",
      "      Else (feature 30 > 0.5)\n",
      "       Predict: 5250000.0\n",
      "     Else (feature 1 > 5.5)\n",
      "      If (feature 2 <= 703.0)\n",
      "       If (feature 29 <= 0.5)\n",
      "        If (feature 2 <= 450.0)\n",
      "         If (feature 2 <= 391.0)\n",
      "          If (feature 3 <= 5.5)\n",
      "           Predict: 1870000.0\n",
      "          Else (feature 3 > 5.5)\n",
      "           Predict: 2200000.0\n",
      "         Else (feature 2 > 391.0)\n",
      "          Predict: 2500000.0\n",
      "        Else (feature 2 > 450.0)\n",
      "         If (feature 0 <= 6.5)\n",
      "          If (feature 3 <= 4.5)\n",
      "           Predict: 3500000.0\n",
      "          Else (feature 3 > 4.5)\n",
      "           Predict: 3704545.4545454546\n",
      "         Else (feature 0 > 6.5)\n",
      "          Predict: 2300000.0\n",
      "       Else (feature 29 > 0.5)\n",
      "        If (feature 0 <= 4.5)\n",
      "         If (feature 4 <= 3.5)\n",
      "          If (feature 2 <= 214.5)\n",
      "           Predict: 1120000.0\n",
      "          Else (feature 2 > 214.5)\n",
      "           Predict: 1900000.0\n",
      "         Else (feature 4 > 3.5)\n",
      "          If (feature 3 <= 2.5)\n",
      "           Predict: 977000.0\n",
      "          Else (feature 3 > 2.5)\n",
      "           Predict: 1465923.076923077\n",
      "        Else (feature 0 > 4.5)\n",
      "         If (feature 20 <= 0.5)\n",
      "          Predict: 2300000.0\n",
      "         Else (feature 20 > 0.5)\n",
      "          Predict: 1800000.0\n",
      "      Else (feature 2 > 703.0)\n",
      "       If (feature 17 <= 0.5)\n",
      "        If (feature 4 <= 22.0)\n",
      "         Predict: 6495000.0\n",
      "        Else (feature 4 > 22.0)\n",
      "         Predict: 6800000.0\n",
      "       Else (feature 17 > 0.5)\n",
      "        If (feature 0 <= 5.5)\n",
      "         Predict: 7450000.0\n",
      "        Else (feature 0 > 5.5)\n",
      "         Predict: 5600000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = labelRDD.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                    numTrees=5, maxDepth=7, maxBins=300, seed=42)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() / float(testData.count())\n",
    "print('Test Mean Squared Error = ' + str(testMSE))\n",
    "print('Learned regression forest model:')\n",
    "print(model.toDebugString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad4a63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce39c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`Neighbordhood Id`' given input columns: [Bathrooms, Date, District Code, District Name, Floor, Increased POP, Increased RFD, Latitude, Longitude, Monthly Price (€/month), Monthly Price Increase, Neighborhood, Neighborhood Code, Neighborhood Id, Operation, POP, Price, Property Code, PropertyType, RFD, Rooms, Size, Surface Price (€/m2), Surface Price Increase, _id];;\n'Project [Rooms#95L, Price#91, 'Neighbordhood Id]\n+- Relation[Bathrooms#75L,Date#76,District Code#77,District Name#78,Floor#79,Increased POP#80,Increased RFD#81,Latitude#82,Longitude#83,Monthly Price (€/month)#84,Monthly Price Increase#85,Neighborhood#86,Neighborhood Code#87,Neighborhood Id#88,Operation#89,POP#90,Price#91,Property Code#92,PropertyType#93,RFD#94,Rooms#95L,Size#96,Surface Price (€/m2)#97,Surface Price Increase#98,_id#99] MongoRelation(MongoRDD[14] at RDD at MongoRDD.scala:51,Some(StructType(StructField(Bathrooms,LongType,true), StructField(Date,StringType,true), StructField(District Code,StringType,true), StructField(District Name,StringType,true), StructField(Floor,StringType,true), StructField(Increased POP,DoubleType,true), StructField(Increased RFD,DoubleType,true), StructField(Latitude,DoubleType,true), StructField(Longitude,DoubleType,true), StructField(Monthly Price (€/month),DoubleType,true), StructField(Monthly Price Increase,DoubleType,true), StructField(Neighborhood,StringType,true), StructField(Neighborhood Code,StringType,true), StructField(Neighborhood Id,StringType,true), StructField(Operation,StringType,true), StructField(POP,DoubleType,true), StructField(Price,DoubleType,true), StructField(Property Code,StringType,true), StructField(PropertyType,StringType,true), StructField(RFD,DoubleType,true), StructField(Rooms,LongType,true), StructField(Size,DoubleType,true), StructField(Surface Price (€/m2),DoubleType,true), StructField(Surface Price Increase,DoubleType,true), StructField(_id,StructType(StructField(oid,StringType,true)),true))))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/efwerr/GitHub/BDM/testing.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000016?line=23'>24</a>\u001b[0m \u001b[39m## --------------- KPI 3 DF --------------- \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000016?line=24'>25</a>\u001b[0m \u001b[39m## --> predict number of rooms given a price and neigbirhood_id\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000016?line=25'>26</a>\u001b[0m dataDF, spark \u001b[39m=\u001b[39m loadMongoDF(db\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mformatted\u001b[39m\u001b[39m'\u001b[39m, collection\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000016?line=26'>27</a>\u001b[0m subsetDF \u001b[39m=\u001b[39m dataDF\u001b[39m.\u001b[39;49mselect(\u001b[39m'\u001b[39;49m\u001b[39mRooms\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mPrice\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mNeighbordhood Id\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000016?line=28'>29</a>\u001b[0m \u001b[39m# Index labels, adding metadata to the label column.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000016?line=29'>30</a>\u001b[0m \u001b[39m# Fit on whole dataset to include all labels in index.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000016?line=30'>31</a>\u001b[0m labelIndexer \u001b[39m=\u001b[39m StringIndexer(inputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRooms\u001b[39m\u001b[39m\"\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindexedRooms\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfit(dataDF)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1421\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[39m@ignore_unicode_prefix\u001b[39m\n\u001b[1;32m   1406\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m1.3\u001b[39m)\n\u001b[1;32m   1407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols):\n\u001b[1;32m   1408\u001b[0m     \u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \n\u001b[1;32m   1410\u001b[0m \u001b[39m    :param cols: list of column names (string) or expressions (:class:`Column`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[39m    [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1421\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   1422\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     raise_from(converted)\n\u001b[1;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`Neighbordhood Id`' given input columns: [Bathrooms, Date, District Code, District Name, Floor, Increased POP, Increased RFD, Latitude, Longitude, Monthly Price (€/month), Monthly Price Increase, Neighborhood, Neighborhood Code, Neighborhood Id, Operation, POP, Price, Property Code, PropertyType, RFD, Rooms, Size, Surface Price (€/m2), Surface Price Increase, _id];;\n'Project [Rooms#95L, Price#91, 'Neighbordhood Id]\n+- Relation[Bathrooms#75L,Date#76,District Code#77,District Name#78,Floor#79,Increased POP#80,Increased RFD#81,Latitude#82,Longitude#83,Monthly Price (€/month)#84,Monthly Price Increase#85,Neighborhood#86,Neighborhood Code#87,Neighborhood Id#88,Operation#89,POP#90,Price#91,Property Code#92,PropertyType#93,RFD#94,Rooms#95L,Size#96,Surface Price (€/m2)#97,Surface Price Increase#98,_id#99] MongoRelation(MongoRDD[14] at RDD at MongoRDD.scala:51,Some(StructType(StructField(Bathrooms,LongType,true), StructField(Date,StringType,true), StructField(District Code,StringType,true), StructField(District Name,StringType,true), StructField(Floor,StringType,true), StructField(Increased POP,DoubleType,true), StructField(Increased RFD,DoubleType,true), StructField(Latitude,DoubleType,true), StructField(Longitude,DoubleType,true), StructField(Monthly Price (€/month),DoubleType,true), StructField(Monthly Price Increase,DoubleType,true), StructField(Neighborhood,StringType,true), StructField(Neighborhood Code,StringType,true), StructField(Neighborhood Id,StringType,true), StructField(Operation,StringType,true), StructField(POP,DoubleType,true), StructField(Price,DoubleType,true), StructField(Property Code,StringType,true), StructField(PropertyType,StringType,true), StructField(RFD,DoubleType,true), StructField(Rooms,LongType,true), StructField(Size,DoubleType,true), StructField(Surface Price (€/m2),DoubleType,true), StructField(Surface Price Increase,DoubleType,true), StructField(_id,StructType(StructField(oid,StringType,true)),true))))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def loadMongoDF(db, collection):\n",
    "    '''\n",
    "    Download data from mongodb and store it in DF format\n",
    "    '''\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(f\"local[*]\") \\\n",
    "        .appName(\"myApp\") \\\n",
    "        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    dataDF = spark.read.format(\"mongo\") \\\n",
    "        .option('uri', f\"mongodb://10.4.41.48/{db}.{collection}\") \\\n",
    "        .load()\n",
    "\n",
    "    return dataDF, spark\n",
    "\n",
    "## --------------- KPI 3 DF --------------- \n",
    "## --> predict number of rooms given a price and neigbirhood_id\n",
    "dataDF, spark = loadMongoDF(db='formatted', collection='data')\n",
    "subsetDF = dataDF.select('Neighborhood Id', 'Price', 'Rooms') \\\n",
    "                .withColumnRenamed(\"Neighborhood Id\",\"Neighborhood_ID\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"Rooms\", outputCol=\"indexedRooms\").fit(subsetDF)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"Neighborhood_ID\", outputCol=\"indexedNeighborhoodID\", maxCategories=4).fit(subsetDF)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingDataDF, testDataDF) = subsetDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedRooms\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingDataDF)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testDataDF)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68e13a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Neighborhood_ID: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Rooms: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/17 02:34:37 ERROR MongoRDD: \n",
      "-----------------------------\n",
      "WARNING: Partitioning failed.\n",
      "-----------------------------\n",
      "\n",
      "Partitioning using the 'DefaultMongoPartitioner$' failed.\n",
      "\n",
      "Please check the stacktrace to determine the cause of the failure or check the Partitioner API documentation.\n",
      "Note: Not all partitioners are suitable for all toplogies and not all partitioners support views.%n\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "22/06/17 02:34:37 WARN DAGScheduler: Creating new stage failed due to exception - job: 7\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n",
      "\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n",
      "\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n",
      "\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n",
      "\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:33)\n",
      "\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:292)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.createShuffleMapStage(DAGScheduler.scala:410)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:379)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:491)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:48)\n",
      "\tat scala.collection.SetLike.map(SetLike.scala:104)\n",
      "\tat scala.collection.SetLike.map$(SetLike.scala:104)\n",
      "\tat scala.collection.mutable.AbstractSet.map(Set.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:490)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:477)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o227.fit.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:33)\n\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:292)\n\tat org.apache.spark.scheduler.DAGScheduler.createShuffleMapStage(DAGScheduler.scala:410)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:379)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:491)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:48)\n\tat scala.collection.SetLike.map(SetLike.scala:104)\n\tat scala.collection.SetLike.map$(SetLike.scala:104)\n\tat scala.collection.mutable.AbstractSet.map(Set.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:490)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:477)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1009)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2940)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2940)\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:241)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/efwerr/GitHub/BDM/testing.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m StringIndexer\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=9'>10</a>\u001b[0m indexers \u001b[39m=\u001b[39m [StringIndexer(inputCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNeighborhood_ID\u001b[39m\u001b[39m'\u001b[39m, outputCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNeighborhood_ID\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_OneHot\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfit(subsetDF) \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mNeighborhood_ID\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=11'>12</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39mindexers)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=12'>13</a>\u001b[0m df_indexed \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mfit(subsetDF)\u001b[39m.\u001b[39mtransform(subsetDF)\n",
      "\u001b[1;32m/Users/efwerr/GitHub/BDM/testing.ipynb Cell 18'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m StringIndexer\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=9'>10</a>\u001b[0m indexers \u001b[39m=\u001b[39m [StringIndexer(inputCol\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNeighborhood_ID\u001b[39;49m\u001b[39m'\u001b[39;49m, outputCol\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNeighborhood_ID\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_OneHot\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(subsetDF) \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mNeighborhood_ID\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=11'>12</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39mindexers)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000019?line=12'>13</a>\u001b[0m df_indexed \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mfit(subsetDF)\u001b[39m.\u001b[39mtransform(subsetDF)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/ml/base.py:129\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    128\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/ml/wrapper.py:321\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> 321\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    322\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/ml/wrapper.py:318\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m:return: fitted Java model\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/sql/utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    130\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o227.fit.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:33)\n\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:292)\n\tat org.apache.spark.scheduler.DAGScheduler.createShuffleMapStage(DAGScheduler.scala:410)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:379)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:491)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:48)\n\tat scala.collection.SetLike.map(SetLike.scala:104)\n\tat scala.collection.SetLike.map$(SetLike.scala:104)\n\tat scala.collection.mutable.AbstractSet.map(Set.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:490)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:477)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1009)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2940)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2940)\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:241)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "subsetDF = dataDF.select('Neighborhood Id', 'Price', 'Rooms') \\\n",
    "                .withColumnRenamed(\"Neighborhood Id\",\"Neighborhood_ID\")\n",
    "subsetDF.printSchema()\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [StringIndexer(inputCol='Neighborhood_ID', outputCol='Neighborhood_ID'+\"_OneHot\").fit(subsetDF) for column in ['Neighborhood_ID']]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(subsetDF).transform(subsetDF)\n",
    "df_indexed.show()\n",
    "\n",
    "#modelDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ffb517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/17 02:13:39 ERROR MongoRDD: \n",
      "-----------------------------\n",
      "WARNING: Partitioning failed.\n",
      "-----------------------------\n",
      "\n",
      "Partitioning using the 'DefaultMongoPartitioner$' failed.\n",
      "\n",
      "Please check the stacktrace to determine the cause of the failure or check the Partitioner API documentation.\n",
      "Note: Not all partitioners are suitable for all toplogies and not all partitioners support views.%n\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "22/06/17 02:13:39 WARN DAGScheduler: Creating new stage failed due to exception - job: 4\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n",
      "\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n",
      "\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n",
      "\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n",
      "\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:33)\n",
      "\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n",
      "\tat org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:292)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.createShuffleMapStage(DAGScheduler.scala:410)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:379)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:491)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:48)\n",
      "\tat scala.collection.SetLike.map(SetLike.scala:104)\n",
      "\tat scala.collection.SetLike.map$(SetLike.scala:104)\n",
      "\tat scala.collection.mutable.AbstractSet.map(Set.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:490)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:477)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o137.fit.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:33)\n\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:292)\n\tat org.apache.spark.scheduler.DAGScheduler.createShuffleMapStage(DAGScheduler.scala:410)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:379)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:491)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:48)\n\tat scala.collection.SetLike.map(SetLike.scala:104)\n\tat scala.collection.SetLike.map$(SetLike.scala:104)\n\tat scala.collection.mutable.AbstractSet.map(Set.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:490)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:477)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1009)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2940)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2940)\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:241)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/efwerr/GitHub/BDM/testing.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/efwerr/GitHub/BDM/testing.ipynb#ch0000020?line=0'>1</a>\u001b[0m labelIndexer \u001b[39m=\u001b[39m StringIndexer(inputCol\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRooms\u001b[39;49m\u001b[39m\"\u001b[39;49m, outputCol\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mindexedRooms\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(subsetDF)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/ml/base.py:129\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    128\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/ml/wrapper.py:321\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> 321\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    322\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/ml/wrapper.py:318\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m:return: fitted Java model\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/pyspark/sql/utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    130\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniforge3/envs/bdm_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o137.fit.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=10.4.41.48:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.partitioner.DefaultMongoPartitioner.partitions(DefaultMongoPartitioner.scala:33)\n\tat com.mongodb.spark.rdd.MongoRDD.getPartitions(MongoRDD.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\n\tat org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:292)\n\tat org.apache.spark.scheduler.DAGScheduler.createShuffleMapStage(DAGScheduler.scala:410)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:379)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:491)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:48)\n\tat scala.collection.SetLike.map(SetLike.scala:104)\n\tat scala.collection.SetLike.map$(SetLike.scala:104)\n\tat scala.collection.mutable.AbstractSet.map(Set.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:490)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:477)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1009)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2940)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2940)\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:241)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"Rooms\", outputCol=\"indexedRooms\").fit(subsetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------- KPI 3 DF --------------- \n",
    "## --> predict number of rooms given a price and neigbirhood_id\n",
    "dataDF, spark = loadMongoDF(db='formatted', collection='data')\n",
    "subsetDF = dataDF.select('Rooms', 'Price', 'Neighbordhood Id')\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"Rooms\", outputCol=\"indexedRooms\").fit(dataDF)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"Neighborhood Id\", outputCol=\"indexedNeighborhoodID\", maxCategories=4).fit(dataDF)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingDataDF, testDataDF) = dataDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingDataDF)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testDataDF)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------- KPI 3 RDD --------------- \n",
    "## --> predict number of rooms given a price and neigbirhood_id\n",
    "rdd_data, spark = loadMongoRDD(db='formatted', collection='data')\n",
    "# we do one-hot-encoding for the categoric variable \"Neighborhood Id\"\n",
    "df1 = rdd_data.map(lambda x: (x['Rooms'], x['Price'], x['Neighborhood Id'])).toDF(['Rooms', 'Price','Neighborhood Id'])\n",
    "categories_id = df1.select('Neighborhood Id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "exprs = [F.when(F.col('Neighborhood Id') == category, 1).otherwise(0).alias(category) for category in categories_id]\n",
    "df2 = df1.select('Rooms','Price', *exprs)\n",
    "# we store the df with the hot encode done to a RDD again\n",
    "rdd3 = df2.rdd\n",
    "\n",
    "# we start the model\n",
    "labelRDD = rdd3.map(lambda x: LabeledPoint(x[0], [x[1:]]))\n",
    "(trainingData, testData) = labelRDD.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "numClasses = len(df1.select('Rooms').distinct().rdd.flatMap(lambda x: x).collect()) + 1\n",
    "model = RandomForest.trainClassifier(trainingData, \n",
    "                                        numClasses=numClasses, \n",
    "                                        categoricalFeaturesInfo={}, \n",
    "                                        numTrees=3, \n",
    "                                        featureSubsetStrategy=\"auto\", \n",
    "                                        impurity='gini', \n",
    "                                        maxDepth=4, \n",
    "                                        maxBins=32)\n",
    "\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "testErr = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "\n",
    "# save the final model\n",
    "spark.stop()\n",
    "sc = SparkContext(appName=\"rf\")\n",
    "model.save(sc, 'exploitation/RFModel')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c46e04696882e2508df5460fc63f94b3349515e9904aee56ac1dfca515397f8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
